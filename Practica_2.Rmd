---
title: "Práctica_2"
author: "Norberto García Marín y María Soledad Pérez López"
date: "3 de enero de 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introducción

El proyecto a realizar trata de un estudio entre una red MLP convencional y una red de convolución. Se realizara una comparación con la ayuda de un conjunto de datos: CIFAR-10.

Nuestra red convolucional está formada por dos tipos de capas: convolución y pooling. Gracias a esto deberíamos obtener mejores resultados durante el aprendizaje en compración con la red MLP, especificamente en el accuracy.

En los siguientes apartados se estudiara el funcionamientos de las tecnicas mecionadas anteriormente y se compararan los resultados para ver si se cumplen nuestras suposiciones sobre el funcionamiento.


### Librerias necesarias

```{r}
library(caret)
library(keras)
```


### Conjunto CIFAR-10

Se cargan los datos y se muestran.

```{r}
set.seed(12345)

dswsize = 10000
dsevalwsize = 2000

cifar10 <- dataset_cifar10()

dssize = 50000
mask = sample(1:dssize,dswsize)

cifar10$train$x = cifar10$train$x[mask,,,]
cifar10$train$y = cifar10$train$y[mask,]

dssizeeval = 10000
mask = sample(1:dssizeeval,dsevalwsize)

cifar10$test$x = cifar10$test$x[mask,,,]
cifar10$test$y = cifar10$test$y[mask,]

str(cifar10)
```


Con esto hemos creado el modelo de datos. Se puede ver que hay dos conjuntos: train y test. El conjunto de entrenamiento está formado por 1000 muestras y tiene imágenes de 32 x 32 (dimesiones) y 3 canales de color RGB, mientras que el test está formado por 2000 muestras.

Se muestran los ejemplos que hay por clase en el conjunto train.

```{r}
barplot(table(cifar10$train$y),main="Tipos de imágenes en cifar10 (train set)",
        xlab="Tipos de imágenes")
```


Y en el conjunto test.

```{r}
barplot(table(cifar10$test$y),main="Tipos de imágenes en cifar10 (test set)",
        xlab="Tipos de imágenes")
```


Se puede observar que existe una proporcionalidad similar entre los ejemplos de ambos conjuntos.

Se aplica una transformación a los conjuntos.

```{r}
x_test <- cifar10$test$x
y_test <- cifar10$test$y

x_train <- cifar10$train$x
y_train <- cifar10$train$y
```


Y ahora hacemos reshape para generar arrays en el formato que usará R. R alamacena las cosas de forma lineal, por lo que la matriz genera 3072 índices, que son las dimensiones del array.

```{r}
x_test = array_reshape(x_test,c(nrow(x_test), 3072))
x_train = array_reshape(x_train,c(nrow(x_train), 3072))
```


Seguidamente, comprobamos la separabilidad de los objetos usando un gráfico PCA. Se usa el grafico PCA el conjunto test y se colorea cada dígito en funcion del ejemplar a representar.

```{r}
n = 10000

pca = prcomp(t(x_train[mask,]))
cos = rainbow(10)
colores = cos[1 + y_train[mask]]

plot(pca$rotation[,1],pca$rotation[,2],col=colores,pch=19,
     xlab="1er PCA",ylab="2o PCA",main=paste0("PCA plot, ",n," imágenes cifar10"))
legend("topright",fill=cos,
       title="Tipos de imágenes",
       col=cos,
       legend=0:9,cex=0.6)
```


Ahora normalizamos los colores en 0 y 1 dividiendo entre el valor 255, que seriá el maximo, para el conjunto x_train y x_test.

```{r}
max(x_test)
max(x_train)

x_test <- x_test / 255
x_train <- x_train / 255
```


Se convierten en datos categóricos y_train y y_test.

```{r}
str(y_train)
y_train[1]
y_train[3]

y_test <- to_categorical(y_test, 10)
y_train <- to_categorical(y_train, 10)


str(y_train)
y_train[1,]
y_train[3,]
```

Ahora, por ejemplo, un tipo de imagen 1 se codifica de la forma: 0 1 0 0 0 0 0 0 0.


# 2. MLP Convencional

Primero debemos crear nuestro modelo secuencial. Las capas de red se definen desde la entrada a la salida, definiendo así la dirección del flujo de datos. Este modelo está definido por capas y la salida de una pasa a las entradas de las siguientes mediante una tubería.

```{r}
model = keras_model_sequential()

model %>%
  layer_dense(units = 32, input_shape = c(3072)) %>%
  layer_activation('relu') %>%
  layer_dense(units = 10) %>%
  layer_activation('softmax')
```

Para empezar, la primera capa toma las entradas de los datos training y test, que generarn vectores de 3072 componentes por entrada. A parte, la capa de salida devuelve 10 posibles salidas, que corresponden a cada tipo de imagen en cuestión.

Además cada nodo de la capa oculta introduce un sesgo que contiene un peso.

En la capa oculta, la función sigmoide recopila información sobre todo pixel de la figura y los procesa.
En la capa de la salida, la función softmax convierte toda la información entre la capa oculta y la de salida en una distribución de probabilidades para los 10 tipos de imágenes.

Por otra parte, en el modelo secuencial se hara uso del operador %% para poder añadir en orden las capas. Se usara layer_dense para conectar los nodos de una capa anterior con la siguiente.

Lanzamos el algoritmo como hemos explicado antes. Vamos a probar distintos hiperparámetros para observar sus resultados. Primero cambiamos el tamaño del bach.

```{r}
bsize = c(16,32,64,128,256,512,1024)

h = 50
nepochs = 50
h_final = 16
acc = 0

accvvals = NULL
acctvals = NULL


for(b in bsize){
    set.seed(12345)

    #Conectar todos los nodos de la capa anterior con los de la siguiente
    model = keras_model_sequential()
    model %>%
      layer_dense(units = h, activation = 'sigmoid', input_shape = c(3072)) %>%
      layer_dense(units = 10, activation = 'softmax')
    
    #Compilar el modelo
    model %>% compile(
      loss = 'categorical_crossentropy',
      optimizer = 'adam',
      metrics = c('accuracy')
    )
    
    #Entrenar el modelo
    history = model %>% fit(
      x_train, y_train,
      epochs = nepochs,
      batch_size = b,
      validation_split = 0.2,
      verbose = 0
    )
    
    cat("Los errores de entrenamiento y evaluación para",b,"batch size son", history$metrics$loss[nepochs],"y",history$metrics$val_loss[nepochs],"\n")
    cat("Valores de accuracy de entrenamiento y evaluación para",b,"batch size son", history$metrics$acc[nepochs],"y",history$metrics$val_acc[nepochs],"\n")
    accvvals = c(accvvals,history$metrics$val_acc[nepochs])
    acctvals = c(acctvals,history$metrics$acc[nepochs])
    if(history$metrics$val_acc[nepochs]>acc){
      acc = history$metrics$val_acc[nepochs]
      b_final = b
    }
}
ymin = min(c(accvvals,acctvals))
plot(y=acctvals,x=bsize,ylab="Accuracy",xlab="batch size",main="Valores de accuracy para entrenamiento/test a través de tamaños de batch",col="blue",ylim=c(ymin,1),type="l")
lines(x=bsize,y=accvvals,col="red")
```


A continuación, cambiamos el número de nodos ocultos.

```{r}
n_hidden = c(48,96,192,256,512)

bsize = b_final
nepochs = 50
h_final = 48
acc = 0

accvvals = NULL
acctvals = NULL

for(h in n_hidden){
  set.seed(12345)
  model = keras_model_sequential()
  model %>%
    layer_dense(units = h, activation = 'sigmoid', input_shape = c(3072)) %>%
    layer_dense(units = 10, activation = 'softmax')
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy')
  )
  history = model %>% fit(
    x_train, y_train,
    epochs = nepochs,
    batch_size = bsize,
    validation_split = 0.2,
    verbose = 0
  )

  cat("Los errores de entrenamiento y evaluación para",h,"nodos son", history$metrics$loss[nepochs],"y", history$metrics$val_loss[nepochs],"\n")
  cat("Los valores de accuracy de entrenamiento y evaluación para",h,"nodos son",history$metrics$acc[nepochs],"y",  history$metrics$val_acc[nepochs],"\n")
  
  accvvals = c(accvvals,history$metrics$val_acc[nepochs])
  acctvals = c(acctvals,history$metrics$acc[nepochs])
  
  if(history$metrics$val_acc[nepochs]>acc){
    acc = history$metrics$val_acc[nepochs]
    h_final = h
  }
}

ymin = min(c(accvvals,acctvals))
plot(y=acctvals,x=n_hidden,ylab="Accuracy",xlab="nodos ocultos",main="Valores de accuracy para entrenamiento/test a través epochs",col="blue",ylim=c(ymin,1),type="l")
lines(x=n_hidden,y=accvvals,col="red")
```

```{r}
cat("Los hiperparámetros escogidos son: bach size",b_final,"y el número de nodos ocultos: ", h_final)
```


## 2.1. Overfitting MLP

El ovverfitting ocurre cuando el Backpropagation se dedica a ajustar la red a pequeñas particularidades locales en los datos de entrenamiento, irrelevantes para el problema general.

Lanzamos la nueva red con los parámetros finales.

```{r}
set.seed(12345)

  model = keras_model_sequential()
  model %>%
    layer_dense(units = h_final, activation = 'sigmoid', input_shape = c(3072)) %>%
    layer_dense(units = 10, activation = 'softmax')
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy')
  )

  history = model %>% fit(
    x_train, y_train,
    epochs = nepochs,
    batch_size = b_final,
    validation_split = 0.2,
    verbose = 0
  )
```


Mostramos gráficamente los errores de entranamiento y validación a lo largo de los epochs.

```{r}
vymax = max(c(history$metrics$loss,history$metrics$val_loss))

plot(history$metrics$loss,main="Erores de entrenamiendo/validación para cifar10",col="blue", type="l",xlab="Epochs",ylab="Loss",ylim=c(0,vymax))
lines(history$metrics$val_loss,col="red")
```

Vemos en los resultados que el error de validación empeora tras un número de epochs concreto. Por lo que hemos cometido overfitting.


Aplicamos varias técnicas para tratar el overfitting

### 2.1.1. L1

#ref =https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c

La técnica L1 de regularización o también llamada Lasso Regression, limita la capacidad de la red actuando en su algoritmo de entrenamiento, añadiendo en la función de perdida un coeficiente de penaliación en función de sus valores abosultos.

```{r}
model_b = keras_model_sequential()
model_b %>%
  layer_dense(units = h_final,
              kernel_regularizer = regularizer_l1(0.001),
              input_shape = c(3072)) %>%
  layer_activation("sigmoid") %>%
  layer_dense(units = 10) %>%
  layer_activation("softmax")
summary(model_b)

model_b %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

history_b = model_b %>% fit(
  x_train, y_train,
  epochs = 50,
  batch_size = b_final,
  validation_split = 0.2,
  verbose = 0)
```


### 2.1.2. L2

La técina L2 de regularización o también llamada Ridge Regression, es una técnica muy parecida a L1 pero esta utiliza los cuadrados en lugar de los valores absolutos.

```{r}
model_c = keras_model_sequential()

model_c %>%
  layer_dense(units = h_final,
              #Podemos usar regularizer_l1, regularizer_l2
              #y regularizer_l1_l2
              kernel_regularizer = regularizer_l2(0.001),
              input_shape = c(3072)) %>%
  layer_activation("sigmoid") %>%
  layer_dense(units = 10) %>%
  layer_activation("softmax")
summary(model_c)

model_c %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

history_c = model_c %>% fit(
  x_train, y_train,
  epochs = 50,
  batch_size = b_final,
  validation_split = 0.2,
  verbose = 0)

```


### 2.1.3. Dropout

Esta técnica consiste en establecer a 0 una fracción del input (establecida al 50%) en cada actualización durante el proceso de entrenamiento, con lo que conseguimos prevenir el overfitting.

```{r}
model_d = keras_model_sequential()

model_d %>%
  layer_dense(units = h_final, activation="sigmoid",
              input_shape = c(3072)) %>%
  layer_dropout(rate=0.5) %>%
  layer_dense(units = 10,activation="softmax") %>%
  summary(model_d)

model_d %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

history_d = model_d %>% fit(
  x_train, y_train,
  epochs = 50,
  batch_size = b_final,
  validation_split = 0.2,
  verbose = 0)
```


Y ahora comparamos los distintos modelos.

```{r}
vymax = max(c(history$metrics$loss,
              history$metrics$val_loss,
              history_b$metrics$loss,
              history_b$metrics$val_loss,
              history_c$metrics$loss,
              history_c$metrics$val_loss,
              history_d$metrics$loss,
              history_d$metrics$val_loss))

plot(history$metrics$loss,main="Training/Validation errors for small cifar10",col="blue",
     type="l",xlab="Epochs",ylab="Loss",ylim=c(1.5,vymax))

lines(history$metrics$val_loss,col="red")
lines(history_b$metrics$loss,col="green")
lines(history_b$metrics$val_loss,col="darkgreen")
lines(history_c$metrics$loss,col="cyan")
lines(history_c$metrics$val_loss,col="darkblue")
lines(history_d$metrics$loss,col="darkviolet")
lines(history_d$metrics$val_loss,col="darkred")
legend("topright",fill=c("blue","red","green","darkgreen",
                         "cyan","darkblue","darkviolet","darkred"),
       title="Regularizaciones",
       col=c("blue","red","green","darkgreen",
             "cyan","darkblue","darkviolet","darkred"),
       legend=c("Training (no reg)",
                "Validation (no reg)",
                "Training (L1)",
                "Validation (L1)",
                "Training (L2)",
                "Validation (L2)",
                "Training (Dropout)",
                "Validation (Dropout)"),cex=0.5)
```


Evaluamos con el conjunto test:

```{r}
results = NULL
results[["noreg"]] = evaluate(model,x_test,y_test)
results[["l1"]] = evaluate(model_b,x_test,y_test)
results[["l2"]] = evaluate(model_c,x_test,y_test)
results[["dropout"]] = evaluate(model_d,x_test,y_test)
accs = unlist(lapply(results,function(x){ return(x$acc)}))
barplot(accs,
        main="Accuracy",names.arg=names(results))
print(accs)
```


### 2.1.4. Crossvalidación para comprobar los resultados

Después de corregir el overfiting hay que asegurar que el modelo mantiene el acurracy mostrado, y para esto utilizamos la crosvalidación.

Es una tecnica utilizada para evaluar los resultados y garantizar que son idenpendientes de la partición entre el conjunto train y test.

```{r}
set.seed(12345)

k=5
nepochs = 50
folds = createFolds(y=cifar10$train$y,k=k)
allindex = 1:length(cifar10$train$y)
accs = accsdrop = NULL

for(i in 1:k){
  eval_index = folds[[i]]
  train_index = allindex[!(allindex %in% eval_index)]
  cat("\nTrabajando en el pliegue",i,"con",length(train_index),"ejemplos de entrenamiento y",length(eval_index),"ejemplos de evaluación\n")
  model_a = keras_model_sequential()
  model_a %>%
    layer_dense(units = h_final, activation = 'sigmoid', input_shape = c(3072)) %>%
    layer_dense(units = 10, activation = 'softmax')
  summary(model_a)
  model_a %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy')
  )
  history = model_a %>% fit(
    x = x_train[train_index,],
    y = y_train[train_index,],
    validation_data = list(x_train[eval_index,],y_train[eval_index,]),
    epochs = nepochs,
    batch_size = b_final,
    validation_split = 0.0,
    verbose = 0
  )
  cat("Nuevo accuracy para MLP, pliegue",i,"=",history$metrics$val_acc[nepochs],"\n")
  accs = c(accs,history$metrics$val_acc[nepochs])
  model_d = keras_model_sequential()
  model_d %>%
    layer_dense(units = h_final, activation="sigmoid",
    input_shape = c(3072)) %>%
    layer_dropout(rate=0.5) %>%
    layer_dense(units = 10,activation="softmax") %>%
  summary(model_d)
  model_d %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'adam',
    metrics = c('accuracy')
  )
  history_d = model_d %>% fit(
    x = x_train[train_index,],
    y = y_train[train_index,],
    validation_data = list(x_train[eval_index,],y_train[eval_index,]),
    epochs = nepochs,
    batch_size = 1024,
    validation_split = 0.0,
    verbose = 0)
  cat("Nuevo accuracy MLP + dropout, pliegue",i,"=",history_d$metrics$val_acc[nepochs],"\n")
  accsdrop = c(accsdrop,history_d$metrics$val_acc[nepochs])
}

cat("El accuracy para MLP es",mean(accs),"\n")
cat("El accuracy para MLP + dropout es",mean(accsdrop),"\n")
```


# 3. Red de convolución

Ahora crearemos una red de convolución (RC) para conseguir mejorar los resultados del caso anterior.

Las redes de convolución son redes neuronales multi-capa, donde es posible representar imágenes de tres dimensiones (anchura, altura y profundidad), la tercera dimensión corresponde a los colores de RGB. Por las tres dimensiones y el tamaño de las imágenes es de 32x32, una sola neurona conectada en una primera capa oculta tendrá 3072 nodos (32x32x3). De esta froma, la información se irá modelando y combinando en capas más profundas de la red. 

Tres capas a destacar de las RC son:

  - Capa convolucional: se reduce el número de entradas obteniendo las consideradas importantes. Utiliza una matriz de pesos que actúa como filtro y genera un mapa de activación.
  
  - Capa de reducción/pooling: se reduce el número de parámetros mediante las caracterésticas más repetidas. Hay que tener en cuenta que cada filtro genera, un mapa de activación por imagen.
  
  - Capa de salida: se encuentra fully connected con la capa anterior, lo que hace incorporar parámetros al modelo y los pesos de conexión de la capa anterior a esta.


## Tratamiento de datos

Volvemos a cargar el conjunto de datos.

```{r}
# Conjunto de datos
set.seed(12345)
dswsize = 10000
dsevalwsize = 2000
cifar10 <- dataset_cifar10()
dssize = 50000
mask = sample(1:dssize,dswsize)
cifar10$train$x = cifar10$train$x[mask,,,]
cifar10$train$y = cifar10$train$y[mask,]
dssizeeval = 10000
mask = sample(1:dssizeeval,dsevalwsize)
cifar10$test$x = cifar10$test$x[mask,,,]
cifar10$test$y = cifar10$test$y[mask,]

# Preparación de datos
batch_size <- 128
num_classes <- 10
nepochs <- 25

# Dimensiones de la imagen
img_rows <- 32
img_cols <- 32

# Obtenemos los conjuntos de entrenamiento y de evaluación.
train_x <- cifar10$train$x
train_y <- cifar10$train$y
test_x <- cifar10$test$x
test_y <- cifar10$test$y

train_x <- array_reshape(train_x, c(nrow(train_x), img_rows, img_cols, 3))
test_x <- array_reshape(test_x, c(nrow(test_x), img_rows, img_cols, 3))
input_shape <- c(img_rows, img_cols, 3)

# Normalizamos los colores en 0 y 1
train_x <- train_x / 255
test_x <- test_x / 255

cat('x_train_shape:', dim(train_x), '\n')
cat(nrow(train_x), 'train samples\n')
cat(nrow(test_x), 'test samples\n')

# Coversión a matrices
train_y <- to_categorical(train_y, num_classes)
test_y <- to_categorical(test_y, num_classes)

```


## 3.1 Definición del modelo

Para crear el modelo vamos a querer tener el control sobre el overfitting, por ello vamos a aplicar dropout.

A la entrada de las neuronas se le pasará el filtro genera un mapa de activación como mencionamos anteriormente. A continuación, usamos una función de activación RELU y también aplicamos activación tipo softmax. 

En la primera capa tenemos 32 filtros con 4x4=16 pesos cada uno, lo que nos resulta en 16x32+32=544 parámetros, 16 pesos por cada filtro y un sesgo por cada filtro.

En la segunda capa utilizamos 64 filtros, con 4x4 = 16 pesos por cada profundidad, 544x64+64=34880 pesos.

De recorrer la imagen original con el filtro obtendremos el map de activación. Este será (a-r+1)x(b-c+1) = (32-4+1)x(32-4+1) = 29x29 = 841, lo que genera un volumen de salida de 29x29x32.

Para saber que hiper-parámetros seleccionar entre los números de nodos ocultos y los tamaños de bach, vamos a ejecutar un bucle donde obtendremos el best_nh, el nodo con mejor accuracy, y con otro bucle comprobaremos que tamaño de bach se ajusta mejor. Una vez escogidos los mejores valores haremos la crossvalidación con esas preferencias.

```{r}
n_hidden = c(48,96,192)
best_acc = 0
best_nh = 48
accvvals2 = NULL
acctvals2 = NULL

for(h in n_hidden){
  model2 <- keras_model_sequential()
  model2 %>%
    layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu',input_shape = input_shape) %>% 
    layer_conv_2d(filters = 64, kernel_size = c(4,4), activation = 'relu') %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_dropout(rate = 0.25) %>% 
    layer_flatten() %>% 
    layer_dense(units = h, activation = 'relu') %>% 
    layer_dropout(rate = 0.5) %>% 
    layer_dense(units = num_classes, activation = 'softmax')
  
  model2 %>% compile(
    loss = loss_categorical_crossentropy,
    optimizer = 'adam',
    metrics = c('accuracy')
  )
  summary(model2)
  
  history2 = model2 %>% fit(
    train_x, train_y,
    batch_size = batch_size,
    epochs = nepochs,
    verbose = 0,
    validation_data = list(test_x, test_y)
  )
  
  scores2 <- model2 %>% evaluate(test_x, test_y, verbose = 0)
  
  
  cat("Errores de entrenamiento y evaluación para ",h," nodos son:", history2$metrics$loss[nepochs], " y ", history2$metrics$val_loss[nepochs],"\n")
  cat("Accuracy de entrenamiento y evaluación para ",h," nodos son:", history2$metrics$acc[nepochs], " y ", history2$metrics$val_acc[nepochs],"\n")
  
  accvvals2 = c(accvvals2,history2$metrics$val_acc[nepochs])
  acctvals2 = c(acctvals2,history2$metrics$acc[nepochs])
  
  # Guardar el nº de nodos que de mejor accuracy
  if(history2$metrics$val_acc[nepochs]>best_acc){
    best_acc = history2$metrics$val_acc[nepochs]
    best_nh = h
  }
}

ymin = min(c(accvvals2,acctvals2))
plot(y=acctvals2,x=n_hidden,ylab="Accuracy",xlab="hidden nodes",main="Accuracy train/test por epochs", col="blue", ylim=c(ymin,1), type="l")
lines(x=n_hidden,y=accvvals2,col="red")
```


Como la prueba de antes era con bsize=128, ahora vamos a ver qué valor de la bsize podría mejorar el accuracy (si es posible).

```{r}
bsize = c(256,512)
acc = 0
best_bsize = 128
acc_bsize = best_acc
accvvals2 = NULL
acctvals2 = NULL

for(b in bsize){
  model2 <- keras_model_sequential()
  model2 %>%
    layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu',input_shape = input_shape) %>% 
    layer_conv_2d(filters = 64, kernel_size = c(4,4), activation = 'relu') %>% 
    layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
    layer_dropout(rate = 0.25) %>% 
    layer_flatten() %>% 
    layer_dense(units = best_nh, activation = 'relu') %>% 
    layer_dropout(rate = 0.5) %>% 
    layer_dense(units = num_classes, activation = 'softmax')
  
  model2 %>% compile( loss = loss_categorical_crossentropy, optimizer = 'adam', metrics = c('accuracy') )
  summary(model2)
  
  history2 = model2 %>% fit(
    train_x, train_y,
    batch_size = b,
    epochs = nepochs,
    verbose = 0,
    validation_data = list(test_x, test_y)
  )
  
  scores2 <- model2 %>% evaluate( test_x, test_y, verbose = 0 )
  
  
  cat("Errores de entrenamiento y evaluación para ", b, " batch size son: " , history2$metrics$loss[nepochs]," y ", history2$metrics$val_loss[nepochs], "\n")
  cat("Accuracy de entrenamiento y evaluación para ", b, " batch size son: ", history2$metrics$acc[nepochs], " y ", history2$metrics$val_acc[nepochs], "\n")
  
  accvvals2 = c(accvvals2,history2$metrics$val_acc[nepochs])
  acctvals2 = c(acctvals2,history2$metrics$acc[nepochs])
  
  if(history2$metrics$val_acc[nepochs]>acc_bsize){
    acc_bsize = history2$metrics$val_acc[nepochs]
    best_bsize = b
  }
}

ymin = min(c(accvvals2,acctvals2))
plot(y=acctvals2, x=bsize, ylab="Accuracy", xlab="batch size", main="Accuracy entrenamiento/test batch size", col="blue", ylim=c(ymin,1), type="l")
lines(x=bsize, y=accvvals2, col="red")
```

```{r}
cat("Hiperparámetros seleccionados: ",best_bsize," bach size y ", best_nh, "nº de nodos ocultos")
```



## 3.2. Crossvalidación para la red

```{r}
k=5
folds = createFolds(y=cifar10$train$y,k=k)
allindex = 1:length(cifar10$train$y)
accs = accsdrop = NULL

for(i in 1:k){
   eval_index = folds[[i]]
   train_index = allindex[!(allindex %in% eval_index)]
   cat("Pliegue ",i," con ",length(train_index), " ejemplos de train y ", length(eval_index),
       " ejemplos de test\n")
   
   model <- keras_model_sequential()
   model %>%
     layer_conv_2d(filters = 32, kernel_size = c(4,4), activation = 'relu',
                   input_shape = input_shape) %>%
     layer_conv_2d(filters = 64, kernel_size = c(4,4), activation = 'relu') %>%
     layer_max_pooling_2d(pool_size = c(2, 2)) %>%
     layer_dropout(rate = 0.25) %>%
     layer_flatten() %>%
     layer_dense(units = best_nh, activation = 'relu') %>%
     layer_dropout(rate = 0.5) %>%
     layer_dense(units = num_classes, activation = 'softmax')
   
   summary(model)
   model %>% compile( loss = loss_categorical_crossentropy, optimizer = 'adam', metrics = c('accuracy') )
   
   history = model %>% fit( train_x, train_y, batch_size=best_bsize, epochs=nepochs, verbose=0,
                            validation_data=list(train_x[eval_index,,,], train_y[eval_index,])
   )
  cat("Accuracy para CNN, pliegue ",i,"=",history$metrics$val_acc[nepochs],"\n")
  accsdrop = c(accsdrop, history$metrics$val_acc[nepochs])
}
cat("Accuracy para la CNN con dropout es ",mean(accsdrop),"\n")
```



# 4. Conclusión

Comparando los resultados obtenidos con un modelo MLP convencional y un modelo de Red de Convolución, hemos podido observar que RC al ser una red más compleja y aunque invierte un poco más de tiempo para la ejecucción, ha podido aprender mejor partiendo de un mismo conjunto de datos ambos modelos.


# 5. Bibliografía

[1] Sesiones de Redes Neuronales y Convolución - Juan A. Botía - Aulavirtual.

[2] *https://github.com/rstudio/keras*

[3] *https://keras.io/*

[4] *https://www.cs.toronto.edu/~kriz/cifar.html*

[5] *https://www.rdocumentation.org/*

[6] *https://medium.com/@ab9.bhatia/set-up-gpu-accelerated-tensorflow-keras-on-windows-10-with-anaconda-e71bfa9506d1*

[7] *https://medium.com/@raza.shahzad/setting-up-tensorflow-gpu-keras-in-conda-on-windows-10-75d4fd498198*
